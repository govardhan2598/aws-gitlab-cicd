# 3. Docker Swarm node Amazon Machine Image (AMI) creation

## 3.1 Swarm node EC2 role creation

We need to allow EC2 instance to use new role. To do that let's create file `ec2-role-trust-policy.json`. The content of the file can be found [here](https://github.com/tikhoplav/aws-gitlab-cicd/blob/master/files/ec2-role-trust-policy.json). Now we will create new EC2 instance role running next command:

```
$ aws iam create-role \
  --role-name swarm-node \
  --assume-role-policy-document file://ec2-role-trust-policy.json \
  --description "Docker Swarm EC2 IAM role. Allows docker images pulling from ECR, and logging to CloudWatch"

{
    "Role": {
        "Path": "/",
        "RoleName": "swarm-node",
        "RoleId": "AROAZ3XCDIOZ64THBEVA5",
        "Arn": "arn:aws:iam::678005261235:role/swarm-node",
        "CreateDate": "2020-04-09T13:50:29+00:00",
        "AssumeRolePolicyDocument": {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": {
                        "Service": "ec2.amazonaws.com"
                    },
                    "Action": "sts:AssumeRole"
                }
            ]
        }
    }
}
```

Next, let's attach required policies to `swarm-node` role:

```
$ aws iam attach-role-policy \
  --role-name swarm-node \
  --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

$ aws iam attach-role-policy \
  --role-name swarm-node \
  --policy-arn arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs
```

<br>

In order to pass this role to the EC2 instance we also have to create instance profile:

```
$ aws iam create-instance-profile --instance-profile-name swarm-node

{
    "InstanceProfile": {
        "Path": "/",
        "InstanceProfileName": "swarm-node",
        "InstanceProfileId": "AIPAZ3XCDIOZWEB5AXHHG",
        "Arn": "arn:aws:iam::678005261235:instance-profile/swarm-node",
        "CreateDate": "2020-04-09T13:58:03+00:00",
        "Roles": []
    }
}
```

And attach it to our EC2 instance role:

```
$ aws iam add-role-to-instance-profile --instance-profile-name swarm-node --role-name swarm-node
```

<br><br>

## 3.2 Launch EC2 instance

```
$ SN=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=production --query "Subnets[*].SubnetId" --output text)

$ SG=$(aws ec2 describe-security-groups --filters Name=group-name,Values=production --query "SecurityGroups[*].GroupId" --output text)
```

<br>

```
$ aws ec2 run-instances \
  --image-id ami-076431be05aaf8080 \
  --instance-type t2.micro \
  --key-name ec2 \
  --security-group-ids $SG \
  --subnet-id "$SN" \
  --iam-instance-profile Name=swarm-node \
  --associate-public-ip-address
```

<details>
<summary>Response</summary>

```
{
    "Groups": [],
    "Instances": [
        {
            "AmiLaunchIndex": 0,
            "ImageId": "ami-076431be05aaf8080",
            "InstanceId": "i-0514c5025fd2cdb16",
            "InstanceType": "t2.micro",
            "KeyName": "ec2",
            "LaunchTime": "2020-04-09T19:07:27+00:00",
            "Monitoring": {
                "State": "disabled"
            },
            "Placement": {
                "AvailabilityZone": "eu-central-1a",
                "GroupName": "",
                "Tenancy": "default"
            },
            "PrivateDnsName": "ip-10-0-0-198.eu-central-1.compute.internal",
            "PrivateIpAddress": "10.0.0.198",
            "ProductCodes": [],
            "PublicDnsName": "",
            "State": {
                "Code": 0,
                "Name": "pending"
            },
            "StateTransitionReason": "",
            "SubnetId": "subnet-05d781c4237ffdf39",
            "VpcId": "vpc-0ee00f62537074796",
            "Architecture": "x86_64",
            "BlockDeviceMappings": [],
            "ClientToken": "",
            "EbsOptimized": false,
            "Hypervisor": "xen",
            "IamInstanceProfile": {
                "Arn": "arn:aws:iam::678005261235:instance-profile/swarm-node",
                "Id": "AIPAZ3XCDIOZWEB5AXHHG"
            },
            "NetworkInterfaces": [
                {
                    "Attachment": {
                        "AttachTime": "2020-04-09T19:07:27+00:00",
                        "AttachmentId": "eni-attach-069f621968d0a7e5d",
                        "DeleteOnTermination": true,
                        "DeviceIndex": 0,
                        "Status": "attaching"
                    },
                    "Description": "",
                    "Groups": [
                        {
                            "GroupName": "production",
                            "GroupId": "sg-0177b3a255b9fc450"
                        }
                    ],
                    "Ipv6Addresses": [],
                    "MacAddress": "02:2e:45:72:77:ee",
                    "NetworkInterfaceId": "eni-02384bdab7a346b18",
                    "OwnerId": "678005261235",
                    "PrivateIpAddress": "10.0.0.198",
                    "PrivateIpAddresses": [
                        {
                            "Primary": true,
                            "PrivateIpAddress": "10.0.0.198"
                        }
                    ],
                    "SourceDestCheck": true,
                    "Status": "in-use",
                    "SubnetId": "subnet-05d781c4237ffdf39",
                    "VpcId": "vpc-0ee00f62537074796",
                    "InterfaceType": "interface"
                }
            ],
            "RootDeviceName": "/dev/xvda",
            "RootDeviceType": "ebs",
            "SecurityGroups": [
                {
                    "GroupName": "production",
                    "GroupId": "sg-0177b3a255b9fc450"
                }
            ],
            "SourceDestCheck": true,
            "StateReason": {
                "Code": "pending",
                "Message": "pending"
            },
            "VirtualizationType": "hvm",
            "CpuOptions": {
                "CoreCount": 1,
                "ThreadsPerCore": 1
            },
            "CapacityReservationSpecification": {
                "CapacityReservationPreference": "open"
            },
            "MetadataOptions": {
                "State": "pending",
                "HttpTokens": "optional",
                "HttpPutResponseHopLimit": 1,
                "HttpEndpoint": "enabled"
            }
        }
    ],
    "OwnerId": "678005261235",
    "ReservationId": "r-0f66d9485c4378f3c"
}
```
</details>

```
$ EC2=i-0514c5025fd2cdb16

$ aws ec2 create-tags --resources $EC2 --tags Key=Name,Value=production-manager \
  && aws ec2 create-tags --resources $EC2 --tags Key=cluster,Value=production \
  && aws ec2 create-tags --resources $EC2 --tags Key=role,Value=manager
```

<br>

```
$ IP=$(aws ec2 describe-instances --instance-ids $EC2 --query "Reservations[*].Instances[*].PublicIpAddress" --output text)
```

<br><br>

## 3.3 EC2 instance confiduration

```
$ ssh -i ~/.ssh/ec2.pem ec2-user@$IP
```

<br>

### 3.3.1 Docker installation

In order to grant access to docker daemon to the CI/CD pipeline, we will need to add `ec2-user` to the `docker` group. Following steps requires to be run as `ec2-user`, so we need to recreate ssh connection after following command sequence: 

```
$ sudo yum -y update \
  && sudo amazon-linux-extras install -y docker \
  && sudo systemctl enable docker \
  && sudo service docker start \
  && sudo usermod -a -G docker ec2-user

$ exit
```

<details>
<summary>To test that docker is running:</summary>

```
$ docker info

Client:
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 1
 Server Version: 19.03.6-ce
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
 Swarm: inactive
 Runtimes: runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: ff48f57fc83a8c44cf4ad5d672424a98ba37ded6
 runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd
 init version: fec3683
 Security Options:
  seccomp
   Profile: default
 Kernel Version: 4.14.171-136.231.amzn2.x86_64
 Operating System: Amazon Linux 2
 OSType: linux
 Architecture: x86_64
 CPUs: 1
 Total Memory: 983.4MiB
 Name: ip-172-31-43-48.eu-central-1.compute.internal
 ID: SC7V:XQZY:PPVK:YIKT:HST2:M4OG:53JH:HLIU:WCWO:SQ6R:3VA4:YAPR
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Registry: https://index.docker.io/v1/
 Labels:
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

```
</details>

<br>

### 3.3.2 Amazon ECR Docker credential helper installation

Before we can push and pull Docker images to ECR, we have to login our Docker daemon to ECR registry. Normally we aquire token that lasts 12 hours with `awc-cli`. In order to let GitLab Runner use ECR constantly we need to do the following steps:

[Amazon ECR Docker Credential Helper](https://github.com/awslabs/amazon-ecr-credential-helper) will allow EC2 instance push and pull images to ECR, since it requires special authentification sequence. Let's install it by running:

```
$ sudo yum install -y amazon-ecr-credential-helper
```

<br>

We will need name of the repository, that stored in ECR. In order to get it, we can use next command *(Make sure to specify correct region)*:

```
$ REGION=eu-central-1
$ REPOSITORY=$(aws ecr describe-repositories --region $REGION --repository-names nginx --query "repositories[*].repositoryUri" --output text)
```

<br>

Now we need to aquire the token and store it with help of credential helper. To achive that we could use [this approach](https://github.com/awslabs/amazon-ecr-credential-helper/issues/63#issuecomment-328318116):

```
$ $(aws ecr get-login --no-include-email --region $REGION) \
  && echo -e "{\n\t\"credsStore\": \"ecr-login\"\n}" | sudo tee ~/.docker/config.json \
  && docker pull $REPOSITORY:alpine
```

After image is downloaded we can make sure that Docker login is cached, by running following command. The answer have to be similar:

```
$ docker-credential-ecr-login list

{"https://678005261235.dkr.ecr.eu-central-1.amazonaws.com":"AWS"}
```

<br>

### 3.3.3 EC2 instance termination sequence registration

Later we will register workers to our swarm using this prototype. We have to add some feature to our prototype, in order to unregister this workers automaticaly when instance is going to be terminated. To achive that we can use [this](https://www.golinuxcloud.com/run-script-with-systemd-before-shutdown-linux/) approach:

```
$ echo -e "#! /bin/bash\ndocker swarm leave --force" | sudo tee /etc/init.d/ec2-terminate.sh \
  && sudo chmod u+x /etc/init.d/ec2-terminate.sh
```

<br>

Next, we will create a `systemd` service, that will run our script at system shutdown:

```
$ sudo vim /etc/systemd/system/ec2-terminate.service
```

With the following content *(`a` to edit file, `esc :x` to save our file and exit)*:

```
[Unit]
Description=EC2 termination sequence
DefaultDependencies=no
Before=shutdown.target

[Service]
Type=oneshot
ExecStart=/etc/init.d/ec2-terminate.sh
TimeoutStartSec=0

[Install]
WantedBy=shutdown.target
```

<br>

Now let's register our new service:

```
$ sudo systemctl enable ec2-terminate.service

Created symlink from /etc/systemd/system/shutdown.target.wants/ec2-terminate.service to /etc/systemd/system/ec2-terminate.service.
```

<br>

### 3.3.4 Clean up

I suggest to clean up an instance, since free tier of ESB volume usage is limited, or if you not using free tier or broke the limit of monthly usage, every single megabyte is charged.

```
$ docker rmi $(sudo docker images -q) \
  && sudo yum clean all \
  && sudo rm -rf /var/cache/yum

$ exit
```

<br><br>

## 3.4 Docker Swarm node EC2 AMI creation

```
$ aws ec2 create-image \
  --instance-id $EC2 \
  --name "Docker Swarm Node" \
  --description "Amazon Linux 2 AMI based. \
  Docker (19.03.6), Amazon ECR Docker credentials helper. \
  Auto leave swarm on termination"

{
    "ImageId": "ami-0901c7867b6172373"
}

$ aws ec2 create-tags --resources ami-0901c7867b6172373 --tags Key=Name,Value=swarm-node
```