# 4. Docker Swarm EC2 cluster configuration

Steps in this section requires id's of some AWS resources that have been created at previous steps so let's get them and save to variables:

```
$ SG=$(aws ec2 describe-security-groups \
	--filters Name=group-name,Values=production \
	--query "SecurityGroups[*].GroupId" \
	--output text)

$ SN=$(aws ec2 describe-subnets \
	--filters Name=tag:Name,Values=production \
	--query "Subnets[*].SubnetId" \
	--output text)

$ AMI=$(aws ec2 describe-images \
	--filters Name=tag:Name,Values=swarm-node \
	--query "Images[*].ImageId" \
	--output text)

$ MIP=$(aws ec2 describe-instances \
	--filters Name=tag:Name,Values=production-manager \
	--query "Reservations[*].Instances[*].PublicIpAddress" \
	--output text)
```

<br><br>

## 4.1 Docker Swarm initialization

Let's get our prototypes public ip address and login with ssh:

```
$ ssh -i ~/.ssh/ec2.pem ec2-user@$MIP
```

<br>

Once we logged in, let's initiate Docker Swarm *(Make sure to copy output, or leave the console opened, we will need this in the next step)*:

```
$ docker swarm init

Swarm initialized: current node (syeua0gytrp3y4k2qn1ul9ymd) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-5lqzkrmd1doqbcczeb8xsvz7s70s5nqiv02t3arymwijc9fcfo-6vacsu0jo5qn6mw3dn2zgcimp 10.0.0.115:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

<br>

Also, let's create a new overlay network, that we will use to connect our services:

```
$ docker network create -d overlay --scope swarm production
```

<br><br>

## 4.2 Swarm node EC2 launch configuration creation

Next step is to create the launch configuration, which serves as a template for future EC2 instances. In order to let our instances join swarm automatically on launch, we will use user-data file. Let's create a file called `swarm-node_user-data.txt` and use [this](https://github.com/tikhoplav/aws-gitlab-cicd/blob/master/files/swarm-node_user-data.txt) as the reference. Make sure you paste correct `token` and `manager's ip address`, that have been outputed in a previous step.

<details>
<summary>Example of the complete user-data file</summary>

```
Content-Type: multipart/mixed; boundary="//"
MIME-Version: 1.0

--//
Content-Type: text/cloud-config; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="cloud-config.txt"

#cloud-config
cloud_final_modules:
- [scripts-user, always]

--//
Content-Type: text/x-shellscript; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="userdata.txt"

#! /bin/bash
docker swarm join --token SWMTKN-1-5lqzkrmd1doqbcczeb8xsvz7s70s5nqiv02t3arymwijc9fcfo-6vacsu0jo5qn6mw3dn2zgcimp 10.0.0.115:2377
--//
```
</details>

<br>

Now we are ready to create a launch configuration:

```
$ aws autoscaling create-launch-configuration \
	--launch-configuration-name production \
	--image-id $AMI \
	--key-name ec2 \
	--security-groups "$SG" \
	--instance-type t2.micro \
	--iam-instance-profile swarm-node \
	--associate-public-ip-address \
	--user-data file://swarm-node_user-data.txt
```

<br><br>

## 4.3 Swarm cluster AWS Auto Scaling Group creation

```
$ aws autoscaling create-auto-scaling-group \
	--auto-scaling-group-name production \
	--launch-configuration-name production \
	--min-size 1 \
	--max-size 30 \
	--desired-capacity 3 \
	--health-check-grace-period 30 \
	--vpc-zone-identifier "$SN" \
	--tags "Key=Name,Value=production-worker,PropagateAtLaunch=true" \
	"Key=cluster,Value=production,PropagateAtLaunch=true" \
	"Key=role,Value=worker,PropagateAtLaunch=true"
```

Now, let's list our instances:

```
$ aws ec2 describe-instances \
	--filter Name=tag:cluster,Values=production \
	--query 'Reservations[*].Instances[*].{
	id:InstanceId,
	public_ip:PublicIpAddress,
	private_ip:PrivateIpAddress,
	status:State.Name,
	tags:Tags[*]}'
```

<details>
<summary>Response example:</summary>

```
[
    [
        {
            "id": "i-035b6f78f78637577",
            "public_ip": "3.125.33.222",
            "private_ip": "10.0.0.115",
            "status": "running",
            "tags": [
                {
                    "Key": "role",
                    "Value": "manager"
                },
                {
                    "Key": "cluster",
                    "Value": "production"
                },
                {
                    "Key": "Name",
                    "Value": "production-manager"
                }
            ]
        }
    ],
    [
        {
            "id": "i-00d40bf18e8034dc2",
            "public_ip": "18.185.149.74",
            "private_ip": "10.0.0.252",
            "status": "running",
            "tags": [
                {
                    "Key": "role",
                    "Value": "worker"
                },
                {
                    "Key": "cluster",
                    "Value": "production"
                },
                {
                    "Key": "aws:autoscaling:groupName",
                    "Value": "production"
                },
                {
                    "Key": "Name",
                    "Value": "production-worker"
                }
            ]
        },
        {
            "id": "i-0d84faf6e30aa8e09",
            "public_ip": "3.125.34.82",
            "private_ip": "10.0.0.41",
            "status": "running",
            "tags": [
                {
                    "Key": "Name",
                    "Value": "production-worker"
                },
                {
                    "Key": "aws:autoscaling:groupName",
                    "Value": "production"
                },
                {
                    "Key": "role",
                    "Value": "worker"
                },
                {
                    "Key": "cluster",
                    "Value": "production"
                }
            ]
        },
        {
            "id": "i-0f43f6fdf59060fdc",
            "public_ip": "18.185.99.169",
            "private_ip": "10.0.0.240",
            "status": "running",
            "tags": [
                {
                    "Key": "role",
                    "Value": "worker"
                },
                {
                    "Key": "Name",
                    "Value": "production-worker"
                },
                {
                    "Key": "aws:autoscaling:groupName",
                    "Value": "production"
                },
                {
                    "Key": "cluster",
                    "Value": "production"
                }
            ]
        }
    ]
]
```
</details>

<br><br>

## 4.5 Docker Swarm cluster on EC2 instances testing

It is time to test that our new cluster is working properly. For this purpose I have prepared two docker images available at DockerHub. Let's start couple of services in our swarm *(Make SSH connection to `production-manager` and run following commands)*. But before let's check our node list:

```
$ docker node ls

ID                            HOSTNAME                                      STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
aev7gfjc00dfwio890c7z4wp5     ip-10-0-0-17.eu-central-1.compute.internal    Ready               Active                                  19.03.6-ce
syeua0gytrp3y4k2qn1ul9ymd *   ip-10-0-0-115.eu-central-1.compute.internal   Ready               Active              Leader              19.03.6-ce
lrx2hf28sgzlyrt82zq7fluvz     ip-10-0-0-118.eu-central-1.compute.internal   Ready               Active                                  19.03.6-ce
qats8cebj2ywo2sfh5yafwtod     ip-10-0-0-236.eu-central-1.compute.internal   Ready               Active                                  19.03.6-ce
```

Now let's start [testing service](https://github.com/tikhoplav/swarm-net-test) *(Make sure to specify right region)*:

```
$ docker service create \
  --name test \
  --network production \
  --replicas 3 \
  --log-driver=awslogs \
  --log-opt awslogs-region=eu-central-1 \
  --log-opt awslogs-group=test \
  --log-opt awslogs-create-group=true \
  tikhoplav/swarm-net-test

8ipcved7eqb6v0y9412wi5aum
overall progress: 3 out of 3 tasks 
1/3: running   
2/3: running   
3/3: running   
verify: Service converged 

$ docker service ls

ID                  NAME                MODE                REPLICAS            IMAGE                             PORTS
8ipcved7eqb6        test                replicated          3/3                 tikhoplav/swarm-net-test:latest   
```

Next service is a slightly [modified nginx](https://github.com/tikhoplav/swarm-nginx-test), that forwards requests to the `test` service *(Three of the containers will start fast, while one of them will take some time. Don't panic, it's perfectly fine, since this contanier is looking for it's upstream at the different ec2 node)*:

```
$ docker service create \
  -p 80:80 \
  --name nginx \
  --network production \
  --mode global \
  --log-driver=awslogs \
  --log-opt awslogs-region=eu-central-1 \
  --log-opt awslogs-group=nginx \
  --log-opt awslogs-create-group=true \
  tikhoplav/swarm-nginx-test

mvr0ii9x6qw8g8ljcavxro484
overall progress: 4 out of 4 tasks 
syeua0gytrp3: running   
r63hz31emoec: running   
oer2ycdbrrf3: running   
owt6df52y0ys: running   
verify: Service converged

$ docker service ls

ID                  NAME                MODE                REPLICAS            IMAGE                               PORTS
mq7ttidy9pmu        nginx               global              4/4                 tikhoplav/swarm-nginx-test:latest   *:80->80/tcp
zqum3zllnxnr        test                replicated          3/3                 tikhoplav/swarm-net-test:latest       
```

<br>

We can test our cluster using saved public ip of the manager. Let's make the following request from the local terminal:

```
$ curl $MIP/test/

{
  "timestamp": "2020-04-10 18:49:17.084938608 +0000 UTC m=+34.295498134",
  "container_id": "7fb5bada0a580a9794bb256035597a5ec539f2c1c3813ed500a8b3ce76708049"
}
```

Try to run this multiple times and see how Docker Swarm integrated load balancer gives us new available node each new request.

<br><br>

## 4.6 Auto Scaling policy attachment

To make our cluster fully autonomous, we need to add feedback to our auto-scaling group. The following command creates a scaling policy and attaches it to our auto-scaling group. What he does is that if the CPU utilization of the EC2 instances drops below 40%, the group shrinks. And if average usage rises above 40%, the auto-scaling group releases new instances. Do not worry, this applies only to the swarm working, as our manager is not controlled by the Auto Scaling group, so you will not lose control over the cluster.

Let's create new file called `scaling-policy.json` with the next content:
```
{
  "PredefinedMetricSpecification": {
    "PredefinedMetricType": "ASGAverageCPUUtilization"
  },
  "TargetValue": 40,
  "DisableScaleIn": false
}
```

Now we are ready to create and apply policy:


```
$ aws autoscaling put-scaling-policy \
  --auto-scaling-group-name production \
  --policy-name production-on-cpu \
  --policy-type TargetTrackingScaling \
  --target-tracking-configuration file://scaling-policy.json
  
{
    "PolicyARN": "arn:aws:autoscaling:eu-central-1:678005261235:scalingPolicy:4ae51a5e-9a0f-434e-a079-6778958a480d:autoScalingGroupName/production:policyName/production-on-cpu",
    "Alarms": [
        {
            "AlarmName": "TargetTracking-production-AlarmHigh-7dbffa52-0765-4af6-a9c2-fe14130ea395",
            "AlarmARN": "arn:aws:cloudwatch:eu-central-1:678005261235:alarm:TargetTracking-production-AlarmHigh-7dbffa52-0765-4af6-a9c2-fe14130ea395"
        },
        {
            "AlarmName": "TargetTracking-production-AlarmLow-81ea8866-fddb-4cf3-a0c3-ad8a8c9ce737",
            "AlarmARN": "arn:aws:cloudwatch:eu-central-1:678005261235:alarm:TargetTracking-production-AlarmLow-81ea8866-fddb-4cf3-a0c3-ad8a8c9ce737"
        }
    ]
}
```